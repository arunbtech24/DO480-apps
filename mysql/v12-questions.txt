					#### EX280 V12 ####
**Order of qurestion might change, you can follow below order. This may help to save time					

01) Manage Identity providers (secret name and identity provider name will be given in the exam
	Create user job with password indionce 
	Create user qwerty with password catalog
	Create user john with password john123
	Create user natasha with password sestivier
	Create user harry with password harry123
	Create user sarah with password sarah123 Identity provider name should be ex280-htpasswd and secret name is ex280-idp-secret

	
02) 02) Manage Cluster Project and Permissions:   ** read qstn carefully - you may need to change answer as per qstn, pattern would be same. Ignore user not found warning
	Create project with named apollo, test,demo
    job user should have cluster administrator rights.
	Sarah user doesn't have cluster access.
	if they ask remove clusterrole from group can create new projects
	Or they may ask sarah user can create project in cluster or natasah user can’t create projects.
	add view role to natasha in Apollo and modify the project in test
	kubeadmin user should not exist (remove kubeadmin user)
	

03) Managing Groups:
	Create a groups with named commander and pilot
	add qwerty user in commander group add harry and sarah users in pilot
	(4q)Give edit permission to commander groups on test project Give view permission to pilot
	groups


04) Create resource quota ex280-quota for project lobster: pod = 3 cpu = 2 services = 6	memory = IGi secrets = 6

	
05) Scale Application Manually:
	scale the single-pod replicas to 4 and make sure all pods should run Successfully in orange.
	
Taints isu if node went into pening state.
	
06) Autoscale of Pods in scaling project:
	minimum replicas=2, maximum replicas=9 and cpu percent 60%
	Default request for container cpu 10m and limit 80m
	


07) Create secret with named magic in secure project The key name should be decode_ring The value of key should be b3BlcmF0b3IK

***** Answer  *****
    oc project secure
	oc create secret generic magic --from-literal decode_ring=b3BlcmF0b3IK -n secure

08) Use Secret in secure project There is one pod already exists. it should use magic secret DECODE_



09)	create a secure route in quart project (scriptfile-given = openssl command are avl =/usr/local/bin/newcert)
	One application is already running named with classified It should run on https with self-signed certificate. It should use subj '


   
 In case passthrough 
    oc describe pod podname | grep Mounts -A2
    oc rsh podname
       pods -- find / -name ssl ,certs search path of the dir
       /etc/pki/tls/cets
	ex: oc create secret tls <secret name> --cert certs/training.crt --key certs/training.key
	oc describe pod podname | grep Mounts -A2    --> it is not mounted we've to mount
	oc set volume dc/dcname --add --name=myvol --type=secret --secret-name=secretname --mount-path=/usr/local/etc/ssl/certs
	oc describe pod podname | grep Mounts -A2
	oc create route passthrough https --service servicename --hostname php.https.apps.ocp4.example.com    (Here while creating route CN name and hostname must be same)
	oc get route
	
10)Create Service Account(user) in project1:
	Service Account (user) Should be ex280-sa
	Service Account should be associated with anyuid SCC.endpoint issue
	Application should produce output

*** Note: there might be one more service account question along with troubleshoot, that case create sa and assign policy and perform troubleshoot
	$ oc project bulky
        $ oc get pods
        $ oc logs pod/podname --> important
        $ oc get events
        $ oc get pods
               $ oc get pod/podname -o yaml | oc adm policy scc-subject-review -f -
        $ oc adm policy add-scc-to-user anyuid  -z ex280-sa
        $ oc set serviceaccount  dc/dcname ex280-sa
        $ oc describe pod console-5df4fcbb47-67c52 -n openshift-console | grep scc
        $ oc  get pods
        $ oc get route 
        $ curl -s routename

11) Install operator
	
***** Answer  *****
	Login to console as admin user (job)
	Operators>OperatorHub , select all project from project drop down
	Search for the operator name asked to instal - "File Integrity Operator"
	Click for install 
	Slect update channel and update approval options mentioned in question 
	Click for install

12) Probe  - Do as per the requirement in question 
	
***** Answer  *****
	oc project orange
	oc edit dc/<dc name>  Or easy method is from GUI - select project -> deployments -> actions -> add health checks 
    
  edit probe is already there, if not put entry under container session - below is example, change as per requirement 
     
		
	
13) Install application using helm chart

***** Answer  *****
	oc project test
	helm repo list
	helm repo add do280-repo http://helm.ocp4.example.com/charts
	helm search repo --versions
 Option 1 - If not asked to mention specific values
	helm install example-app  do280-repo/etherpad --version 0.0.6
 Option 2 - if there are values to be edited
	vi value.yaml
		image:
			repository: registry.ocp4.example.com:8443/etherpad
			name: etherpad
			tag: 1.8.18
		route:
		   host: development-etherpad.apps.ocp4.example.com
	helm install example-app do280-repo/etherpad -f values.yaml --version 0.0.6

14)	Create another resource limit file "ex280-limits" for defining range of resource access based on soft/hard limit in sydney project.
	i. For pods min cpu limit is "5m" and max is "500m"
	ii. For containers min cpu limit is "100m" and max is "500m" and default request of "300m"
	iii. For pods min memory is "300Mi" and max is "500Mi"
	iv. For containers min memory is "200Mi" and max is "600Mi" and default request of "400Mi"
	
	
	oc create -f ex280-limits.yaml -n sydney
	oc describe limitranges


	
15) PV and PVC - Will ask to create pv, pvc and mount to deployment

***** Answer  *****
	oc project orange
	oc get sc  - to get storage class
	Crate PV as per requirement from GUI 
	Create PVC as per requirement from GUI using yaml (refer existing pv of image registry, use storage class and volumeMode) - volume mode is important 
	oc create deployment <dn> --image <> --port <>  (Or use oc new-app --name <app name> --image .... command, remember if any -e for variable if required)
	oc set volume deployment/orange --add --name nfs-volume --type pvc --claim-class nfs-storage --claim-mode rwo --claim-size 1Gi --claim-name nfs-pvc --mount-path /var/www/html
	
16) Mult or Create network policy 


If the question is to create network policy - this is the question asked in exam
	
	*** We may need to create deny all policy first, check access from node of any project to the namspace pod  use rsh method mentioned below) , read the question carefully 
	
	Ex: Create a network policy to allow traffic to the hello pod in the network-policy namespace from the sample-app pod in the network-test namespace over TCP on port 8080
	
	 


17) Create bootstrap template 

***** Answer  *****
	oc edit clusterrolebinding self-provisioners (*** Only if user/group need to chenage)
		modify as per requirement in session 
				subjects:
				- apiGroup: rbac.authorization.k8s.io
				  kind: Group
				  name: provisioners
	
	oc adm create-bootstrap-project-template -o yaml > template.yaml
	vi template.yaml (edit and put limit entries above parmeters as per requirement)
	- apiVersion: v1
	  kind: LimitRange
	  metadata: 
	     name: max-memory
		 namespace: ${PROJECT_NAME}
	  spec:
	    limits:
		- default:
		    memory: 1Gi
		  defaultRequest
		    memory: 1Gi
		  max:
		    memory: 2Gi
		  type: Container
		  
	oc create -f template.yaml -n openshift-config
	oc edit projects.config.openshift.io cluster
		spec:
			projectRequestTemplate:
			    name: project-request
		  
18) cronjob - 

In GUI - Go to project, under workloads - cronjobs - create cronjob,  edit as per requirement and click create (Take care about all important values including schedule,image, env, etc)

**example of yaml 

apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner   (edit required)
  namespace: workshop-support   (automatically selected if in right project)
spec:
  schedule: "*/1 * * * *"     (here it is every minute, update as per requirement)
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never    (edit required)
          serviceAccountName: project-cleaner-sa  
          containers:
            - name: project-cleaner  (edit required)
              image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.1  (edit required)
              imagePullPolicy: Always
              env:     (this part is important)
              - name: "PROJECT_TAG"
                value: "workshop"
              - name: "EXPIRATION_SECONDS"
                value: "10"
              resources:   (if asked)
                limits:
                  cpu: 100m
                  memory: 200Mi

19) must-gather

***** Answer  *****
	oc get clusterversion -o yaml | grep -i clusterid
	oc adm must-gather
	tar cavf <name.tar.gz>  must-gather.....
	upload using script provided
	
  ** in case for specific feature 
    oc adm must-gather --image-stream=openshift/must-gather --image=quay.io/kubevirt/must-gather
	
20) config map - practice config map with key and value (--from-liteal)
    

#### Troubleshooting questions

Common comands
	oc get pods - pods will be running already running, service issue *** here pods will be up and running 
	oc logs frontend-57b8b445df-f56qh (The output does not indicate any errors.)
	
	
	oc get events - check events for warining
	oc get events | grep <pod name>  --> nodes had taint that pod din't tolerate - Perform taint trouble shoot 
	
	oc get pods - Check pod state - It will be in pending state  - Node selector - 
	oc describe pod hello-ts-5dbff9f44-8h7c7  -  there will be waring fail scheduling and observe Node selector 
	
	$oc get events | grep  podname 
	( The issue was 5 insufficient cpu already inside deployment they have given memory as 80Gi change that to 80Mi)
	


21) Application trouble shoot - pods will be running already running, service issue *** here pods will be up and running 
	Inspect the pod logs for errors. The output does not indicate any errors.
		oc logs frontend-57b8b445df-f56qh
			App is ready at : 8080
	List the services in the project and ensure that the frontend service exists.
		oc get svc
			NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
			frontend   ClusterIP   172.30.23.147   <none>        8080/TCP    93m
			mysql      ClusterIP   172.30.103.29   <none>        3306/TCP    93m
	Review the configuration and status of the frontend service. Notice the value of the service selector that indicates to which pod the service should forward packets.
		oc describe svc/frontend
			Annotations:       <none>
			Selector:          name=api
			Type:              ClusterIP
			IP:                172.30.23.147
			Port:              <unset>  8080/TCP
			TargetPort:        8080/TCP
			Endpoints:         <none>
	Notice that there is no endpoint for selector name=api
	Retrieve the labels of the frontend deployment. The output shows that pods are created with a name label that has a value of frontend, whereas the service in the previous step uses api as the value.
		oc describe deployment/frontend | grep -A1 Labels
			Labels:                 app=todonodejs
                        name=frontend
			--
			  Labels:  app=todonodejs
					   name=frontend
	Run the oc edit command to edit the frontend service. Update the selector to match the correct label.
		oc edit svc/frontend
			selector:
			name: frontend
	Review the service configuration to ensure that the service has an endpoint. (end point update automatically when selector updated)
		oc describe svc/frontend
			Selector:          name=frontend	

22)* Taint 
Deploy an application called rocky((app1) pro -p1)) and at last it should be accessed by the following link
 http://rocky.apps.domain7.com  

	oc get nodes
	oc describe nodes | grep -i taints
	oc get dc
	oc edit dc/dcname
	spec:
            dnsPolicy: ClusterFirst
            tolerations:
            - effect: NoSchedule
              key: node
              operator: Equal
              value: worker
	oc get pods
    oc get svc
    oc expose svc/svcname --hostname rocky.apps.domain7.com
    oc get route
    curl http://rocky.apps.domain7.com

23) Node selector issue 
Deploy application in the project mercury There is one pod already running and,
    Application should produce output
    

***** Answer  *****
	Check pod state - It will be in pending state 
		oc get pods
	run oc describe pod command, there will be a warning as Warning  FailedScheduling  ...  0/3 nodes are available: 3 node(s) didn't match node selector or Pod's node affinity selector.
			also there will be a Node-Selector: client=acme (example)
		oc describe pod hello-ts-5dbff9f44-8h7c7 
	
	Check whether it is labled or not labled - This will show the value, case sensitive
		oc get nodes -L client
	edit deploymnet and put correct value in label field 
			nodeSelector:
				client: ACME
		
  If they ask don’t make any changes in resources	
	check in deployment nodeSelector for node label
		oc get deployment/<dn> -o yaml | grep -i selector -A1
	There will be a label for node selecrtor - ex: client: acme
	
	apply that label to node
		ex: oc label node master01 client=acme --overwrite (ex: oc label node master01 star=Trek --overwrite)
	If there is no label in deployment, remove label
		oc label node master01 client-
		
		oc get nodes --show-labels to validate
	oc get all - to make sure pode is up and running 

  If there is no Node selector, you may remove label from Node also
  

Q24). Deploy an application in cherry project named ronik
      
      $oc get events | grep  podname
       ( The issue was 5 insufficient cpu already inside deployment they have given memory as 80Gi change that to 80Mi)
      $ oc desc deployment/dcname | grep -A 5 resources -- identifiy the values 100Gi
      $ oc edit deployment/deploymentname

        resources:
          requests:
            memory: 80Gi --> to 80Mi 
      $ oc get pods     
        
***Tips**
oc delete all -l app=postgresql-persistent2 (to delete a deployment and related resources)

oc get pods -o wide  - will get details inclusding IP and where the pod is running 

oc get resourcequota 0 to get quota details

oc create deployment loadtest --dry-run=client --image quay.io/redhattraining/loadtest:v1.0 -o yaml > ~/DO280/labs/schedule-review/loadtest.yaml  - for creating a deploymnet yaml 
