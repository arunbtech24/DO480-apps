					#### EX280 V12 ####
**Order of qurestion might change, you can follow below order. This may help to save time					

01) Manage Identity providers (secret name and identity provider name will be given in the exam
	Create user job with password indionce 
	Create user qwerty with password catalog
	Create user john with password john123
	Create user natasha with password sestivier
	Create user harry with password harry123
	Create user sarah with password sarah123 Identity provider name should be ex280-htpasswd and secret name is ex280-idp-secret

***** Answer *******
	htpasswd -c -b -B ex280-htpasswd job indionce
	htpasswd -b  ex280-htpasswd qwerty catalog
	htpasswd -b  ex280-htpasswd john john123
	htpasswd -b  ex280-htpasswd natasha sestivier
	htpasswd -b  ex280-htpasswd harry harry123
	htpasswd -b  ex280-htpasswd sarah sarah123
	
	oc create secret generic ex280-idp-secret --from-file htpasswd=ex280-htpasswd -n openshift-config
	
	oc get oauth cluster -o yaml > oauth.yaml
	vi oauth.yaml
		spec:
		  identityProviders:
		     - htpasswd:
			      fileData:
				     name: ex280-idp-secret
		       mappingMethod: claim
			   name: ex280-htpasswd
			   type: HTPasswd
	oc replace -f oauth.yaml
	
02) 02) Manage Cluster Project and Permissions:   ** read qstn carefully - you may need to change answer as per qstn, pattern would be same. Ignore user not found warning
	Create project with named apollo, test,demo
    job user should have cluster administrator rights.
	Sarah user doesn't have cluster access.
	if they ask remove clusterrole from group can create new projects
	Or they may ask sarah user can create project in cluster or natasah user can’t create projects.
	add view role to natasha in Apollo and modify the project in test
	kubeadmin user should not exist (remove kubeadmin user)
	
****** Answer **************
	oc new-project apollo
	oc new-project test
	oc new-project demo
	oc adm policy add-cluster-role-to-user cluster-admin job
	oc adm policy add-cluster-role-to-user self-provisioner sarah
	oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
	(Note:- we have removed selfprovisioner role from group so that all users can't able to create project as per question
              only xxx user cannot create so for other users add self-provsioner role ).
	oc describe clusterrolebindings self-provisioners  (to validate)
	oc adm policy add-cluster-role-to-user self-provisioner <un other than self provisioner not required>  - Perform for all users apart from user not required self provisioner 
	oc policy add-role-to-user view natasha -n apollo
	oc policy add-role-to-user edit natasha -n test
	oc login -u job -p indionce    (make sure cluster admin role applied)
	oc delete secret kubeadmin -n kube-system

03) Managing Groups:
	Create a groups with named commander and pilot
	add qwerty user in commander group add harry and sarah users in pilot
	(4q)Give edit permission to commander groups on test project Give view permission to pilot
	groups

***** Answer  *****
	oc adm groups new commander
	oc adm groups new pilot
	oc adm groups add-users commander qwerty
	oc adm groups add-users pilot harry
	oc adm groups add-users pilot sarah
	oc policy add-role-to-group edit commander -n test
	oc policy add-role-to-group view pilot -n test

04) Create resource quota ex280-quota for project lobster: pod = 3 cpu = 2 services = 6	memory = IGi secrets = 6

***** Answer  *****
	oc project projectname
	oc create quota ex280-quota --hard pods=3,cpu=2,services=6,memory=1Gi,secrets=6 -n lobster
	oc describe quota ex280-quota
	
05) Scale Application Manually:
	scale the single-pod replicas to 4 and make sure all pods should run Successfully in orange.
	
***** Answer  *****
	oc project orange
	oc get all   (to get deployment name)
	oc scale --replicas=4 deployment/orange  (notedown deployment name 	and give)
  If the pods are in pending state 
	oc get pods --> all pods are pending state because of taints so set tolerations
        oc get dc
        oc edit dc/dcname 
        spec:
          dnsPolicy: ClusterFirst
          tolerations:
          - effect: NoSchedule
            key: node
            operator: Equal
            value: worker
	
06) Autoscale of Pods in scaling project:
	minimum replicas=2, maximum replicas=9 and cpu percent 60%
	Default request for container cpu 10m and limit 80m
	
***** Answer  *****
    oc project scaling
	oc get all (to get deployment name)
	oc set resources deployment/scaling --requests cpu=10m --limits cpu=80m
	oc get deployment/scaling -o yaml to validate
	oc autoscale deployment/scaling --max=9 --min=2 --cpu-percent=60
	oc get all to check autoscaling or oc get hpa

07) Create secret with named magic in secure project The key name should be decode_ring The value of key should be b3BlcmF0b3IK

***** Answer  *****
    oc project secure
	oc create secret generic magic --from-literal decode_ring=b3BlcmF0b3IK -n secure

08) Use Secret in secure project There is one pod already exists. it should use magic secret DECODE_

***** Answer  *****
	oc project projectname
	oc get events | grep <podname>
	oc logs podname | head -n2
	oc get dc
	oc get all (to get deployment)
	oc set env deployment/secure --from secret/magic --prefix=DECODE_ (observer prefix name and give that, if without _ then use that)
	oc get pod
	oc get pods
    oc rsh podname  - for testing only 
    oc get route 
    curl -s routename

09)	create a secure route in quart project (scriptfile-given = openssl command are avl =/usr/local/bin/newcert)
	One application is already running named with classified It should run on https with self-signed certificate. It should use subj '

***** Answer  *****
	crete certificate using the script provided 	
	oc project quart
	oc get all to get service name
	oc create route edge <route name> --service= --cert=<.cert> --key=<.key> --hostname= <CN>
	From the terminal, use the curl command with the -I and -v options to retrieve the connection headers.
		ex: curl -I -v https://todo-https.apps.ocp4.example.com
   
 In case passthrough 
    oc describe pod podname | grep Mounts -A2
    oc rsh podname
       pods -- find / -name ssl ,certs search path of the dir
       /etc/pki/tls/cets
	ex: oc create secret tls <secret name> --cert certs/training.crt --key certs/training.key
	oc describe pod podname | grep Mounts -A2    --> it is not mounted we've to mount
	oc set volume dc/dcname --add --name=myvol --type=secret --secret-name=secretname --mount-path=/usr/local/etc/ssl/certs
	oc describe pod podname | grep Mounts -A2
	oc create route passthrough https --service servicename --hostname php.https.apps.ocp4.example.com    (Here while creating route CN name and hostname must be same)
	oc get route
	
10)Create Service Account(user) in project1:
	Service Account (user) Should be ex280-sa
	Service Account should be associated with anyuid SCC.endpoint issue
	Application should produce output
	
***** Answer  *****
	oc create serviceaccount ex280-sa -n project1
	oc adm policy add-scc-to-user anyuid -z ex280-sa
	oc set sa deployment/<dn> ex280-sa
	oc describe pod/<pod name>
	oc get all

*** Note: there might be one more service account question along with troubleshoot, that case create sa and assign policy and perform troubleshoot
	$ oc project bulky
        $ oc get pods
        $ oc logs pod/podname --> important
        $ oc get events
        $ oc get pods
               $ oc get pod/podname -o yaml | oc adm policy scc-subject-review -f -
        $ oc adm policy add-scc-to-user anyuid  -z ex280-sa
        $ oc set serviceaccount  dc/dcname ex280-sa
        $ oc describe pod console-5df4fcbb47-67c52 -n openshift-console | grep scc
        $ oc  get pods
        $ oc get route 
        $ curl -s routename

11) Install operator
	
***** Answer  *****
	Login to console as admin user (job)
	Operators>OperatorHub , select all project from project drop down
	Search for the operator name asked to instal - "File Integrity Operator"
	Click for install 
	Slect update channel and update approval options mentioned in question 
	Click for install

12) Probe  - Do as per the requirement in question 
	
***** Answer  *****
	oc project orange
	oc edit dc/<dc name>  Or easy method is from GUI - select project -> deployments -> actions -> add health checks 
    
  edit probe is already there, if not put entry under container session - below is example, change as per requirement 
     readinessProbe:
          httpGet:
            port: 8080
            path: /readyz
            scheme: HTTPS
     ivenessProbe:
          httpGet:
            port: 8080
            path: /livez
            scheme: HTTPS
     startupProbe:
          httpGet:
            path: /readyz
            port: 8080
            scheme: HTTPS
      failureThreshold: 30
      periodSeconds: 3
	  initialDelaySeconds: 120
	  successThreshold: 1
	  timeoutSeconds: 10
	  tcpSocket:
            port: 9090
		
	
13) Install application using helm chart

***** Answer  *****
	oc project test
	helm repo list
	helm repo add do280-repo http://helm.ocp4.example.com/charts
	helm search repo --versions
 Option 1 - If not asked to mention specific values
	helm install example-app  do280-repo/etherpad --version 0.0.6
 Option 2 - if there are values to be edited
	vi value.yaml
		image:
			repository: registry.ocp4.example.com:8443/etherpad
			name: etherpad
			tag: 1.8.18
		route:
		   host: development-etherpad.apps.ocp4.example.com
	helm install example-app do280-repo/etherpad -f values.yaml --version 0.0.6

14)	Create another resource limit file "ex280-limits" for defining range of resource access based on soft/hard limit in sydney project.
	i. For pods min cpu limit is "5m" and max is "500m"
	ii. For containers min cpu limit is "100m" and max is "500m" and default request of "300m"
	iii. For pods min memory is "300Mi" and max is "500Mi"
	iv. For containers min memory is "200Mi" and max is "600Mi" and default request of "400Mi"
	
***** Answer  *****
	oc project sydney
	vi ex280-limits.yaml
		apiVersion: v1
		kind: LimitRange
		metadata:
		   name: ex280-limits
		spec:
		  limits:
		  - type: Pod
		    max:
			  memory: 500Mi
			  cpu: 500m
			min:
			  memory: 300Mi
			  cpu: 5m
		 -  type: Container
		    max:
			  memory: 600Mi
			  cpu: 500m
			min:
			  memory: 200Mi
			  cpu: 100m
			defaultRequest:
			   memory: 400Mi
			   cpu: 300m
	
	oc create -f ex280-limits.yaml -n sydney
	oc describe limitranges


	
15) PV and PVC - Will ask to create pv, pvc and mount to deployment

***** Answer  *****
	oc project orange
	oc get sc  - to get storage class
	Crate PV as per requirement from GUI 
	Create PVC as per requirement from GUI using yaml (refer existing pv of image registry, use storage class and volumeMode) - volume mode is important 
	oc create deployment <dn> --image <> --port <>  (Or use oc new-app --name <app name> --image .... command, remember if any -e for variable if required)
	oc set volume deployment/orange --add --name nfs-volume --type pvc --claim-class nfs-storage --claim-mode rwo --claim-size 1Gi --claim-name nfs-pvc --mount-path /var/www/html
	
16) Mult or Create network policy 


If the question is to create network policy - this is the question asked in exam
	
	*** We may need to create deny all policy first, check access from node of any project to the namspace pod  use rsh method mentioned below) , read the question carefully 
	
	Ex: Create a network policy to allow traffic to the hello pod in the network-policy namespace from the sample-app pod in the network-test namespace over TCP on port 8080
	
	Create a yaml file ex: allow-specific.yaml 
		kind: NetworkPolicy
		apiVersion: networking.k8s.io/v1
		metadata:
			name: allow-specific
		spec:
		  podSelector:
		    matchLabels:
			  deployment: hello
		  ingress:
		    - from:
			    - namespaceSelector:
				    matchLabels:
					   name: network-test
				  podSelector:
				    matchLabels:
					  deployment: sample-app
			  ports:
			  - port: 8080
			    protocol: tcp
	oc create -f allow-specific.yaml
	oc get networkpolicies -n network-policy
  As the admin user, label the network-test namespace with the name=network-test label.
	oc login -u admin -p redhat
	oc label namespace network-test name=network-test
	
  Verify that the sample-app pod can access the IP address of the hello pod, but cannot access the IP address of the test pod
	oc project network-test
	oc rsh sample-app-d5f945-spx9q curl 10.8.0.13:8080 | grep Hello  


17) Create bootstrap template 

***** Answer  *****
	oc edit clusterrolebinding self-provisioners (*** Only if user/group need to chenage)
		modify as per requirement in session 
				subjects:
				- apiGroup: rbac.authorization.k8s.io
				  kind: Group
				  name: provisioners
	
	oc adm create-bootstrap-project-template -o yaml > template.yaml
	vi template.yaml (edit and put limit entries above parmeters as per requirement)
	- apiVersion: v1
	  kind: LimitRange
	  metadata: 
	     name: max-memory
		 namespace: ${PROJECT_NAME}
	  spec:
	    limits:
		- default:
		    memory: 1Gi
		  defaultRequest
		    memory: 1Gi
		  max:
		    memory: 2Gi
		  type: Container
		  
	oc create -f template.yaml -n openshift-config
	oc edit projects.config.openshift.io cluster
		spec:
			projectRequestTemplate:
			    name: project-request
		  
18) cronjob - 

In GUI - Go to project, under workloads - cronjobs - create cronjob,  edit as per requirement and click create (Take care about all important values including schedule,image, env, etc)

**example of yaml 

apiVersion: batch/v1
kind: CronJob
metadata:
  name: project-cleaner   (edit required)
  namespace: workshop-support   (automatically selected if in right project)
spec:
  schedule: "*/1 * * * *"     (here it is every minute, update as per requirement)
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: Never    (edit required)
          serviceAccountName: project-cleaner-sa  
          containers:
            - name: project-cleaner  (edit required)
              image: registry.ocp4.example.com:8443/redhattraining/do280-project-cleaner:v1.1  (edit required)
              imagePullPolicy: Always
              env:     (this part is important)
              - name: "PROJECT_TAG"
                value: "workshop"
              - name: "EXPIRATION_SECONDS"
                value: "10"
              resources:   (if asked)
                limits:
                  cpu: 100m
                  memory: 200Mi

19) must-gather

***** Answer  *****
	oc get clusterversion -o yaml | grep -i clusterid
	oc adm must-gather
	tar cavf <name.tar.gz>  must-gather.....
	upload using script provided
	
  ** in case for specific feature 
    oc adm must-gather --image-stream=openshift/must-gather --image=quay.io/kubevirt/must-gather
	
20) config map - practice config map with key and value (--from-liteal)
    

#### Troubleshooting questions

Common comands
	oc get pods - pods will be running already running, service issue *** here pods will be up and running 
	oc logs frontend-57b8b445df-f56qh (The output does not indicate any errors.)
	
	
	oc get events - check events for warining
	oc get events | grep <pod name>  --> nodes had taint that pod din't tolerate - Perform taint trouble shoot 
	
	oc get pods - Check pod state - It will be in pending state  - Node selector - 
	oc describe pod hello-ts-5dbff9f44-8h7c7  -  there will be waring fail scheduling and observe Node selector 
	
	$oc get events | grep  podname 
	( The issue was 5 insufficient cpu already inside deployment they have given memory as 80Gi change that to 80Mi)
	


21) Application trouble shoot - pods will be running already running, service issue *** here pods will be up and running 
	Inspect the pod logs for errors. The output does not indicate any errors.
		oc logs frontend-57b8b445df-f56qh
			App is ready at : 8080
	List the services in the project and ensure that the frontend service exists.
		oc get svc
			NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
			frontend   ClusterIP   172.30.23.147   <none>        8080/TCP    93m
			mysql      ClusterIP   172.30.103.29   <none>        3306/TCP    93m
	Review the configuration and status of the frontend service. Notice the value of the service selector that indicates to which pod the service should forward packets.
		oc describe svc/frontend
			Annotations:       <none>
			Selector:          name=api
			Type:              ClusterIP
			IP:                172.30.23.147
			Port:              <unset>  8080/TCP
			TargetPort:        8080/TCP
			Endpoints:         <none>
	Notice that there is no endpoint for selector name=api
	Retrieve the labels of the frontend deployment. The output shows that pods are created with a name label that has a value of frontend, whereas the service in the previous step uses api as the value.
		oc describe deployment/frontend | grep -A1 Labels
			Labels:                 app=todonodejs
                        name=frontend
			--
			  Labels:  app=todonodejs
					   name=frontend
	Run the oc edit command to edit the frontend service. Update the selector to match the correct label.
		oc edit svc/frontend
			selector:
			name: frontend
	Review the service configuration to ensure that the service has an endpoint. (end point update automatically when selector updated)
		oc describe svc/frontend
			Selector:          name=frontend	

22)* Taint 
Deploy an application called rocky((app1) pro -p1)) and at last it should be accessed by the following link
 http://rocky.apps.domain7.com  

	oc get nodes
	oc describe nodes | grep -i taints
	oc get dc
	oc edit dc/dcname
	spec:
            dnsPolicy: ClusterFirst
            tolerations:
            - effect: NoSchedule
              key: node
              operator: Equal
              value: worker
	oc get pods
    oc get svc
    oc expose svc/svcname --hostname rocky.apps.domain7.com
    oc get route
    curl http://rocky.apps.domain7.com

23) Node selector issue 
Deploy application in the project mercury There is one pod already running and,
    Application should produce output
    

***** Answer  *****
	Check pod state - It will be in pending state 
		oc get pods
	run oc describe pod command, there will be a warning as Warning  FailedScheduling  ...  0/3 nodes are available: 3 node(s) didn't match node selector or Pod's node affinity selector.
			also there will be a Node-Selector: client=acme (example)
		oc describe pod hello-ts-5dbff9f44-8h7c7 
	
	Check whether it is labled or not labled - This will show the value, case sensitive
		oc get nodes -L client
	edit deploymnet and put correct value in label field 
			nodeSelector:
				client: ACME
		
  If they ask don’t make any changes in resources	
	check in deployment nodeSelector for node label
		oc get deployment/<dn> -o yaml | grep -i selector -A1
	There will be a label for node selecrtor - ex: client: acme
	
	apply that label to node
		ex: oc label node master01 client=acme --overwrite (ex: oc label node master01 star=Trek --overwrite)
	If there is no label in deployment, remove label
		oc label node master01 client-
		
		oc get nodes --show-labels to validate
	oc get all - to make sure pode is up and running 

  If there is no Node selector, you may remove label from Node also
  

Q24). Deploy an application in cherry project named ronik
      
      $oc get events | grep  podname
       ( The issue was 5 insufficient cpu already inside deployment they have given memory as 80Gi change that to 80Mi)
      $ oc desc deployment/dcname | grep -A 5 resources -- identifiy the values 100Gi
      $ oc edit deployment/deploymentname

        resources:
          requests:
            memory: 80Gi --> to 80Mi 
      $ oc get pods     
        
***Tips**
oc delete all -l app=postgresql-persistent2 (to delete a deployment and related resources)

oc get pods -o wide  - will get details inclusding IP and where the pod is running 

oc get resourcequota 0 to get quota details

oc create deployment loadtest --dry-run=client --image quay.io/redhattraining/loadtest:v1.0 -o yaml > ~/DO280/labs/schedule-review/loadtest.yaml  - for creating a deploymnet yaml 
